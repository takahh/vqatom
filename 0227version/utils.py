import numpy as np
import torch
import logging
import pytz
import random
import os
import yaml
import shutil
from datetime import datetime
from ogb.nodeproppred import Evaluator
from dgl import function as fn

CPF_data = ["cora", "citeseer", "pubmed", "a-computer", "a-photo"]
OGB_data = ["ogbn-arxiv", "ogbn-products"]
NonHom_data = ["pokec", "penn94"]
BGNN_data = ["house_class", "vk_class"]
CORE_ELEMENTS = {"5", "6", "7", "8", "14", "15", "16"}


CBDICT = {
    '16_0_3_0_0_1_0_0_0_0': 11,  # SS=0.4362, N=105, old=11
    '16_0_3_1_1_0_6_3_0_0': 11,  # SS=0.3911, N=193, old=11
    '16_0_3_1_1_2_5_2_1_0': 23,  # SS=0.2765, N=376, old=23
    '16_0_3_1_1_2_5_2_2_0': 22,  # SS=0.3178, N=390, old=22
    '16_0_3_1_1_2_5_2_3_0': 18,  # SS=0.3196, N=242, old=18
    '16_0_3_1_1_2_5_2_4_0': 12,  # SS=0.3996, N=93, old=12
    '16_0_4_0_0_0_8_1_0_0': 15,  # SS=0.4574, N=381, old=15
    '16_0_4_0_0_2_0_0_0_0': 11,  # SS=0.4437, N=82, old=11
    '16_0_4_0_0_2_0_1_0_0': 19,  # SS=0.3193, N=247, old=19
    '16_0_4_0_0_4_0_0_0_0': 23,  # SS=0.3476, N=414, old=23
    '16_0_4_0_0_4_0_1_0_0': 36,  # SS=0.2663, N=987, old=36
    '16_0_4_0_0_4_0_2_0_0': 12,  # SS=0.3455, N=100, old=12
    '6_0_2_0_0_0_6_2_0_0': 13,  # SS=0.3675, N=288, old=13
    '6_0_2_0_0_0_7_1_0_0': 11,  # SS=0.3976, N=191, old=11
    '6_0_2_0_0_2_0_0_0_0': 23,  # SS=0.3171, N=383, old=23
    '6_0_2_0_0_2_0_1_0_0': 32,  # SS=0.3208, N=769, old=32
    '6_0_3_0_0_0_6_2_0_0': 19,  # SS=0.3559, N=653, old=19
    '6_0_3_0_0_0_6_3_0_0': 14,  # SS=0.3263, N=325, old=14
    '6_0_3_0_0_0_7_1_0_0': 12,  # SS=0.5345, N=247, old=12
    '6_0_3_0_0_0_8_1_0_0': 41,  # SS=0.3420, N=3106, old=41
    '6_0_3_0_0_1_0_0_0_0': 12,  # SS=0.6813, N=110, old=12
    '6_0_3_0_0_1_8_1_0_0': 10,  # SS=0.2214, N=174, old=10
    '6_0_3_0_0_2_0_0_0_0': 30,  # SS=0.3833, N=663, old=30
    '6_0_3_0_0_2_0_1_0_0': 26,  # SS=0.2596, N=489, old=26
    '6_0_3_0_0_2_6_2_0_0': 10,  # SS=0.3637, N=135, old=10
    '6_0_3_0_0_2_8_1_0_0': 18,  # SS=0.1997, N=605, old=18
    '6_0_3_0_0_3_0_0_0_0': 176,  # SS=0.2148, N=6009, old=88
    '6_0_3_0_0_3_0_1_0_0': 67,  # SS=0.2040, N=3288, old=67
    '6_0_3_0_0_3_0_2_0_0': 14,  # SS=0.3240, N=142, old=14
    '6_0_3_0_0_3_8_1_0_0': 15,  # SS=0.2111, N=382, old=15
    '6_0_3_0_1_0_6_2_0_0': 11,  # SS=0.4537, N=202, old=11
    '6_0_3_0_1_0_6_3_0_0': 12,  # SS=0.4751, N=246, old=12
    '6_0_3_0_1_0_8_1_0_0': 24,  # SS=0.4065, N=879, old=24
    '6_0_3_0_1_2_6_0_1_0': 11,  # SS=0.3359, N=129, old=11
    '6_0_3_0_1_2_8_1_0_0': 10,  # SS=0.2325, N=177, old=10
    '6_0_3_0_1_3_5_0_1_0': 26,  # SS=0.2629, N=513, old=26
    '6_0_3_0_1_3_5_0_2_0': 17,  # SS=0.2828, N=225, old=17
    '6_0_3_0_1_3_5_0_3_0': 15,  # SS=0.3224, N=180, old=15
    '6_0_3_0_1_3_5_1_1_0': 17,  # SS=0.2937, N=218, old=17
    '6_0_3_0_1_3_5_1_2_0': 13,  # SS=0.3586, N=125, old=13
    '6_0_3_0_1_3_6_0_1_0': 26,  # SS=0.2320, N=533, old=26
    '6_0_3_0_1_3_6_0_2_0': 15,  # SS=0.3058, N=194, old=15
    '6_0_3_0_1_3_6_0_3_0': 13,  # SS=0.3119, N=114, old=13
    '6_0_3_0_1_3_6_1_1_0': 13,  # SS=0.2727, N=151, old=13
    '6_0_3_0_1_3_6_1_2_0': 10,  # SS=0.3950, N=79, old=10
    '6_0_3_0_1_3_8_0_1_0': 17,  # SS=0.3848, N=198, old=17
    '6_0_3_1_1_0_0_0_0_0': 33,  # SS=0.5916, N=2040, old=33
    '6_0_3_1_1_0_6_1_0_0': 17,  # SS=0.4440, N=604, old=17
    '6_0_3_1_1_0_6_2_0_0': 129,  # SS=0.3642, N=29467, old=129
    '6_0_3_1_1_0_6_3_0_0': 92,  # SS=0.3487, N=15128, old=92
    '6_0_3_1_1_0_6_4_0_0': 19,  # SS=0.4143, N=668, old=19
    '6_0_3_1_1_0_7_1_0_0': 13,  # SS=0.5083, N=261, old=13
    '6_0_3_1_1_0_7_2_0_0': 56,  # SS=0.3728, N=5428, old=56
    '6_0_3_1_1_0_7_3_0_0': 25,  # SS=0.4016, N=1167, old=25
    '6_0_3_1_1_0_8_1_0_0': 45,  # SS=0.4075, N=3773, old=45
    '6_0_3_1_1_0_8_2_0_0': 34,  # SS=0.3690, N=2044, old=34
    '6_0_3_1_1_0_8_4_0_0': 11,  # SS=0.4234, N=239, old=11
    '6_0_3_1_1_1_6_2_0_0': 30,  # SS=0.1903, N=1644, old=30
    '6_0_3_1_1_1_6_3_0_0': 22,  # SS=0.2229, N=799, old=22
    '6_0_3_1_1_1_7_2_0_0': 13,  # SS=0.2646, N=297, old=13
    '6_0_3_1_1_1_8_1_0_0': 11,  # SS=0.2701, N=182, old=11
    '6_0_3_1_1_2_0_0_0_1': 14,  # SS=0.2615, N=452, old=14
    '6_0_3_1_1_2_5_2_1_1': 45,  # SS=0.2411, N=1489, old=45
    '6_0_3_1_1_2_5_2_2_1': 49,  # SS=0.2216, N=1792, old=49
    '6_0_3_1_1_2_5_2_3_1': 40,  # SS=0.2401, N=1204, old=40
    '6_0_3_1_1_2_5_2_4_1': 30,  # SS=0.2819, N=707, old=30
    '6_0_3_1_1_2_5_2_5_1': 11,  # SS=0.5073, N=109, old=11
    '6_0_3_1_1_2_6_2_0_1': 116,  # SS=0.1582, N=6133, old=58
    '6_0_3_1_1_2_6_2_1_1': 342,  # SS=0.1917, N=22676, old=171
    '6_0_3_1_1_2_6_2_2_1': 342,  # SS=0.1870, N=22032, old=171
    '6_0_3_1_1_2_6_2_3_1': 302,  # SS=0.2126, N=17351, old=151
    '6_0_3_1_1_2_6_2_4_1': 100,  # SS=0.2366, N=7725, old=100
    '6_0_3_1_1_2_6_2_5_1': 45,  # SS=0.2563, N=1550, old=45
    '6_0_3_1_1_2_6_2_6_1': 22,  # SS=0.3177, N=339, old=22
    '6_0_3_1_1_2_6_2_7_1': 11,  # SS=0.4830, N=96, old=11
    '6_0_3_1_1_2_6_3_0_1': 42,  # SS=0.1579, N=3074, old=42
    '6_0_3_1_1_2_7_2_0_1': 24,  # SS=0.2082, N=1157, old=24
    '6_0_3_1_1_2_7_3_0_1': 12,  # SS=0.2984, N=239, old=12
    '6_0_3_1_1_2_8_1_0_1': 22,  # SS=0.1926, N=782, old=22
    '6_0_3_1_1_2_8_2_0_1': 15,  # SS=0.2281, N=392, old=15
    '6_0_3_1_1_3_0_0_0_0': 11,  # SS=0.2491, N=275, old=11
    '6_0_3_1_1_3_5_2_1_0': 66,  # SS=0.1960, N=3348, old=66
    '6_0_3_1_1_3_5_2_2_0': 53,  # SS=0.2099, N=2162, old=53
    '6_0_3_1_1_3_5_2_3_0': 41,  # SS=0.2248, N=1209, old=41
    '6_0_3_1_1_3_5_2_4_0': 23,  # SS=0.2809, N=418, old=23
    '6_0_3_1_1_3_5_3_1_0': 66,  # SS=0.1839, N=3364, old=66
    '6_0_3_1_1_3_5_3_2_0': 63,  # SS=0.1851, N=2942, old=63
    '6_0_3_1_1_3_5_3_3_0': 42,  # SS=0.2171, N=1387, old=42
    '6_0_3_1_1_3_5_3_4_0': 25,  # SS=0.2915, N=528, old=25
    '6_0_3_1_1_3_5_3_5_0': 11,  # SS=0.4598, N=92, old=11
    '6_0_3_1_1_3_6_2_0_0': 45,  # SS=0.1676, N=3694, old=45
    '6_0_3_1_1_3_6_2_1_0': 278,  # SS=0.1710, N=14624, old=139
    '6_0_3_1_1_3_6_2_2_0': 240,  # SS=0.1708, N=10983, old=120
    '6_0_3_1_1_3_6_2_3_0': 194,  # SS=0.1898, N=7175, old=97
    '6_0_3_1_1_3_6_2_4_0': 59,  # SS=0.2256, N=2680, old=59
    '6_0_3_1_1_3_6_2_5_0': 27,  # SS=0.2788, N=540, old=27
    '6_0_3_1_1_3_6_2_6_0': 16,  # SS=0.4081, N=124, old=16
    '6_0_3_1_1_3_6_3_0_0': 31,  # SS=0.1831, N=1895, old=31
    '6_0_3_1_1_3_6_3_1_0': 65,  # SS=0.1848, N=3291, old=65
    '6_0_3_1_1_3_6_3_2_0': 67,  # SS=0.2038, N=3335, old=67
    '6_0_3_1_1_3_6_3_3_0': 49,  # SS=0.2039, N=1999, old=49
    '6_0_3_1_1_3_6_3_4_0': 32,  # SS=0.2733, N=822, old=32
    '6_0_3_1_1_3_6_3_5_0': 13,  # SS=0.2774, N=144, old=13
    '6_0_3_1_1_3_7_2_0_0': 18,  # SS=0.2153, N=656, old=18
    '6_0_3_1_1_3_8_1_0_0': 16,  # SS=0.2417, N=451, old=16
    '6_0_3_1_1_3_8_2_0_0': 12,  # SS=0.2382, N=265, old=12
    '6_0_4_0_0_0_0_0_0_0': 13,  # SS=0.6491, N=253, old=13
    '6_0_4_0_0_0_6_1_0_0': 29,  # SS=0.4048, N=1560, old=29
    '6_0_4_0_0_0_6_2_0_0': 77,  # SS=0.2835, N=10841, old=77
    '6_0_4_0_0_0_6_3_0_0': 90,  # SS=0.2789, N=14174, old=90
    '6_0_4_0_0_0_6_4_0_0': 34,  # SS=0.3001, N=2061, old=34
    '6_0_4_0_0_0_7_1_0_0': 11,  # SS=0.4773, N=177, old=11
    '6_0_4_0_0_0_7_2_0_0': 31,  # SS=0.3235, N=1717, old=31
    '6_0_4_0_0_0_7_3_0_0': 47,  # SS=0.2712, N=3842, old=47
    '6_0_4_0_0_0_8_1_0_0': 27,  # SS=0.4265, N=1291, old=27
    '6_0_4_0_0_0_8_2_0_0': 66,  # SS=0.2896, N=7544, old=66
    '6_0_4_0_0_0_8_4_0_0': 15,  # SS=0.3728, N=484, old=15
    '6_0_4_0_0_1_0_0_0_0': 130,  # SS=0.3175, N=13021, old=130
    '6_0_4_0_0_1_0_1_0_0': 63,  # SS=0.2620, N=3055, old=63
    '6_0_4_0_0_1_6_2_0_0': 29,  # SS=0.1935, N=1642, old=29
    '6_0_4_0_0_1_6_3_0_0': 38,  # SS=0.2002, N=2420, old=38
    '6_0_4_0_0_1_6_4_0_0': 3,  # SS=0.2512, N=401, old=3
    '6_0_4_0_0_1_7_2_0_0': 12,  # SS=0.2534, N=242, old=12
    '6_0_4_0_0_1_7_3_0_0': 21,  # SS=0.2240, N=760, old=21
    '6_0_4_0_0_1_8_2_0_0': 29,  # SS=0.2911, N=1466, old=29
    '6_0_4_0_0_2_0_0_0_0': 258,  # SS=0.2152, N=13042, old=129
    '6_0_4_0_0_2_0_1_0_0': 166,  # SS=0.1970, N=5187, old=83
    '6_0_4_0_0_2_0_2_0_0': 28,  # SS=0.2209, N=583, old=28
    '6_0_4_0_0_2_6_1_0_0': 14,  # SS=0.2119, N=303, old=14
    '6_0_4_0_0_2_6_2_0_0': 36,  # SS=0.1977, N=2304, old=36
    '6_0_4_0_0_2_6_3_0_0': 42,  # SS=0.2387, N=3096, old=42
    '6_0_4_0_0_2_6_4_0_0': 15,  # SS=0.3040, N=450, old=15
    '6_0_4_0_0_2_7_2_0_0': 12,  # SS=0.2745, N=337, old=12
    '6_0_4_0_0_2_7_3_0_0': 22,  # SS=0.2224, N=856, old=22
    '6_0_4_0_0_2_8_1_0_0': 11,  # SS=0.2562, N=261, old=11
    '6_0_4_0_0_2_8_2_0_0': 29,  # SS=0.3022, N=1625, old=29
    '6_0_4_0_0_3_0_0_0_0': 60,  # SS=0.2355, N=2837, old=60
    '6_0_4_0_0_3_0_1_0_0': 35,  # SS=0.2422, N=949, old=35
    '6_0_4_0_0_3_0_2_0_0': 13,  # SS=0.3085, N=152, old=13
    '6_0_4_0_0_3_6_2_0_0': 26,  # SS=0.2234, N=1356, old=26
    '6_0_4_0_0_3_6_3_0_0': 33,  # SS=0.2325, N=1789, old=33
    '6_0_4_0_0_3_6_4_0_0': 13,  # SS=0.3145, N=277, old=13
    '6_0_4_0_0_3_7_2_0_0': 11,  # SS=0.3029, N=186, old=11
    '6_0_4_0_0_3_7_3_0_0': 16,  # SS=0.2818, N=476, old=16
    '6_0_4_0_0_3_8_1_0_0': 12,  # SS=0.2796, N=172, old=12
    '6_0_4_0_0_3_8_2_0_0': 23,  # SS=0.3594, N=912, old=23
    '6_0_4_0_0_4_0_0_0_0': 34,  # SS=0.4373, N=906, old=34
    '6_0_4_0_0_4_0_1_0_0': 37,  # SS=0.3180, N=1038, old=37
    '6_0_4_0_1_0_0_0_0_0': 23,  # SS=0.6519, N=985, old=23
    '6_0_4_0_1_0_6_1_0_0': 14,  # SS=0.4673, N=323, old=14
    '6_0_4_0_1_0_6_2_0_0': 73,  # SS=0.4001, N=9598, old=73
    '6_0_4_0_1_0_6_3_0_0': 42,  # SS=0.3802, N=3103, old=42
    '6_0_4_0_1_0_6_4_0_0': 15,  # SS=0.4965, N=363, old=15
    '6_0_4_0_1_0_7_2_0_0': 20,  # SS=0.3951, N=668, old=20
    '6_0_4_0_1_0_7_3_0_0': 28,  # SS=0.3786, N=1404, old=28
    '6_0_4_0_1_0_8_1_0_0': 18,  # SS=0.4814, N=529, old=18
    '6_0_4_0_1_0_8_2_0_0': 24,  # SS=0.4311, N=877, old=24
    '6_0_4_0_1_1_6_2_0_0': 15,  # SS=0.1951, N=484, old=15
    '6_0_4_0_1_2_0_0_0_0': 11,  # SS=0.1924, N=214, old=11
    '6_0_4_0_1_2_3_0_1_0': 15,  # SS=0.6059, N=156, old=15
    '6_0_4_0_1_2_3_0_2_0': 16,  # SS=0.3898, N=201, old=16
    '6_0_4_0_1_2_3_0_3_0': 25,  # SS=0.4772, N=382, old=25
    '6_0_4_0_1_2_3_0_4_0': 22,  # SS=0.5201, N=336, old=22
    '6_0_4_0_1_2_3_0_5_0': 10,  # SS=0.5395, N=100, old=10
    '6_0_4_0_1_2_4_0_1_0': 12,  # SS=0.4051, N=110, old=12
    '6_0_4_0_1_2_4_0_2_0': 16,  # SS=0.4384, N=204, old=16
    '6_0_4_0_1_2_4_0_3_0': 21,  # SS=0.3984, N=311, old=21
    '6_0_4_0_1_2_4_0_4_0': 11,  # SS=0.3898, N=121, old=11
    '6_0_4_0_1_2_5_0_1_0': 41,  # SS=0.2661, N=1351, old=41
    '6_0_4_0_1_2_5_0_2_0': 42,  # SS=0.2606, N=1301, old=42
    '6_0_4_0_1_2_5_0_3_0': 38,  # SS=0.2895, N=1196, old=38
    '6_0_4_0_1_2_5_0_4_0': 30,  # SS=0.3813, N=660, old=30
    '6_0_4_0_1_2_5_0_5_0': 17,  # SS=0.4314, N=176, old=17
    '6_0_4_0_1_2_5_1_1_0': 14,  # SS=0.3136, N=179, old=14
    '6_0_4_0_1_2_5_1_2_0': 13,  # SS=0.4234, N=125, old=13
    '6_0_4_0_1_2_5_1_3_0': 10,  # SS=0.3615, N=97, old=10
    '6_0_4_0_1_2_6_0_1_0': 78,  # SS=0.2186, N=4777, old=78
    '6_0_4_0_1_2_6_0_2_0': 84,  # SS=0.2288, N=5648, old=84
    '6_0_4_0_1_2_6_0_3_0': 74,  # SS=0.2564, N=4095, old=74
    '6_0_4_0_1_2_6_0_4_0': 50,  # SS=0.2900, N=2164, old=50
    '6_0_4_0_1_2_6_0_5_0': 26,  # SS=0.3982, N=439, old=26
    '6_0_4_0_1_2_6_1_1_0': 28,  # SS=0.2334, N=564, old=28
    '6_0_4_0_1_2_6_1_2_0': 22,  # SS=0.3105, N=384, old=22
    '6_0_4_0_1_2_6_1_3_0': 14,  # SS=0.3116, N=140, old=14
    '6_0_4_0_1_2_6_2_0_0': 33,  # SS=0.1756, N=1939, old=33
    '6_0_4_0_1_2_6_3_0_0': 19,  # SS=0.1780, N=660, old=19
    '6_0_4_0_1_2_7_0_1_0': 18,  # SS=0.3053, N=237, old=18
    '6_0_4_0_1_2_7_0_2_0': 16,  # SS=0.3839, N=194, old=16
    '6_0_4_0_1_2_7_0_3_0': 18,  # SS=0.3223, N=195, old=18
    '6_0_4_0_1_2_7_0_4_0': 13,  # SS=0.4625, N=126, old=13
    '6_0_4_0_1_2_7_1_1_0': 12,  # SS=0.3745, N=106, old=12
    '6_0_4_0_1_2_7_3_0_0': 13,  # SS=0.2451, N=262, old=13
    '6_0_4_0_1_2_8_0_1_0': 20,  # SS=0.3219, N=311, old=20
    '6_0_4_0_1_3_3_0_1_0': 14,  # SS=0.4517, N=169, old=14
    '6_0_4_0_1_3_3_0_2_0': 14,  # SS=0.3779, N=112, old=14
    '6_0_4_0_1_3_3_0_3_0': 16,  # SS=0.4022, N=153, old=16
    '6_0_4_0_1_3_3_0_4_0': 11,  # SS=0.4907, N=95, old=11
    '6_0_4_0_1_3_4_0_1_0': 11,  # SS=0.4147, N=81, old=11
    '6_0_4_0_1_3_4_0_3_0': 10,  # SS=0.4435, N=62, old=10
    '6_0_4_0_1_3_5_0_1_0': 36,  # SS=0.2343, N=979, old=36
    '6_0_4_0_1_3_5_0_2_0': 27,  # SS=0.2948, N=596, old=27
    '6_0_4_0_1_3_5_0_3_0': 23,  # SS=0.3563, N=390, old=23
    '6_0_4_0_1_3_5_0_4_0': 16,  # SS=0.2866, N=159, old=16
    '6_0_4_0_1_3_5_1_1_0': 16,  # SS=0.3520, N=182, old=16
    '6_0_4_0_1_3_5_1_2_0': 12,  # SS=0.3648, N=128, old=12
    '6_0_4_0_1_3_5_1_3_0': 11,  # SS=0.3904, N=95, old=11
    '6_0_4_0_1_3_6_0_1_0': 44,  # SS=0.2127, N=1427, old=44
    '6_0_4_0_1_3_6_0_2_0': 38,  # SS=0.2500, N=1125, old=38
    '6_0_4_0_1_3_6_0_3_0': 30,  # SS=0.2908, N=724, old=30
    '6_0_4_0_1_3_6_0_4_0': 18,  # SS=0.3166, N=305, old=18
    '6_0_4_0_1_3_6_0_5_0': 10,  # SS=0.5617, N=62, old=10
    '6_0_4_0_1_3_6_1_1_0': 21,  # SS=0.3065, N=332, old=21
    '6_0_4_0_1_3_6_1_2_0': 17,  # SS=0.2758, N=205, old=17
    '6_0_4_0_1_3_6_1_3_0': 15,  # SS=0.2992, N=166, old=15
    '6_0_4_0_1_3_6_2_0_0': 24,  # SS=0.1926, N=1115, old=24
    '6_0_4_0_1_3_6_3_0_0': 13,  # SS=0.2630, N=384, old=13
    '6_0_4_0_1_3_7_0_1_0': 10,  # SS=0.3937, N=99, old=10
    '6_0_4_0_1_3_8_0_1_0': 18,  # SS=0.3504, N=219, old=18
    '6_0_4_0_1_4_5_0_1_0': 17,  # SS=0.3345, N=237, old=17
    '6_0_4_0_1_4_5_1_1_0': 12,  # SS=0.3761, N=109, old=12
    '6_0_4_0_1_4_5_1_2_0': 10,  # SS=0.4309, N=84, old=10
    '6_0_4_0_1_4_6_0_1_0': 20,  # SS=0.3143, N=282, old=20
    '6_0_4_0_1_4_6_0_2_0': 15,  # SS=0.3173, N=196, old=15
    '6_0_4_0_1_4_6_0_3_0': 10,  # SS=0.4838, N=81, old=10
    '6_0_4_0_1_4_6_1_1_0': 11,  # SS=0.4280, N=100, old=11
    '6_0_4_0_1_4_6_1_3_0': 11,  # SS=0.4119, N=59, old=11
    '7_0_2_0_0_0_6_2_0_0': 26,  # SS=0.3433, N=1192, old=26
    '7_0_2_0_0_1_0_0_0_0': 31,  # SS=0.4324, N=658, old=31
    '7_0_2_0_0_1_6_2_0_0': 13,  # SS=0.3270, N=232, old=13
    '7_0_2_0_0_2_6_2_0_0': 11,  # SS=0.3404, N=248, old=11
    '7_0_3_0_0_0_6_2_0_0': 17,  # SS=0.4371, N=566, old=17
    '7_0_3_0_0_0_6_3_0_0': 49,  # SS=0.3627, N=4389, old=49
    '7_0_3_0_0_0_7_2_0_0': 12,  # SS=0.4856, N=227, old=12
    '7_0_3_0_0_0_8_1_0_0': 11,  # SS=0.5192, N=265, old=11
    '7_0_3_0_0_1_0_0_0_0': 36,  # SS=0.4763, N=958, old=36
    '7_0_3_0_0_1_0_1_0_0': 32,  # SS=0.3067, N=781, old=32
    '7_0_3_0_0_1_6_3_0_0': 16,  # SS=0.3219, N=498, old=16
    '7_0_3_0_0_2_0_0_0_0': 72,  # SS=0.2131, N=3799, old=72
    '7_0_3_0_0_2_0_1_0_0': 65,  # SS=0.2189, N=3207, old=65
    '7_0_3_0_0_2_0_2_0_0': 34,  # SS=0.2400, N=881, old=34
    '7_0_3_0_0_2_6_3_0_0': 23,  # SS=0.2836, N=915, old=23
    '7_0_3_0_0_3_0_0_0_0': 22,  # SS=0.2758, N=396, old=22
    '7_0_3_0_0_3_0_1_0_0': 22,  # SS=0.3304, N=353, old=22
    '7_0_3_0_0_3_6_3_0_0': 16,  # SS=0.3071, N=539, old=16
    '7_0_3_0_1_0_6_2_0_0': 25,  # SS=0.3936, N=1056, old=25
    '7_0_3_0_1_0_6_3_0_0': 24,  # SS=0.4325, N=1059, old=24
    '7_0_3_0_1_2_5_0_1_0': 15,  # SS=0.3912, N=167, old=15
    '7_0_3_0_1_2_5_0_2_0': 11,  # SS=0.3815, N=91, old=11
    '7_0_3_0_1_2_6_0_1_0': 13,  # SS=0.3863, N=134, old=13
    '7_0_3_0_1_2_6_2_0_0': 13,  # SS=0.2642, N=234, old=13
    '7_0_3_0_1_2_6_3_0_0': 12,  # SS=0.3057, N=215, old=12
    '7_0_3_0_1_2_8_0_1_0': 15,  # SS=0.3119, N=148, old=15
    '7_0_3_0_1_3_5_0_1_0': 18,  # SS=0.2660, N=286, old=18
    '7_0_3_0_1_3_5_0_2_0': 16,  # SS=0.2922, N=213, old=16
    '7_0_3_0_1_3_5_0_3_0': 12,  # SS=0.2961, N=145, old=12
    '7_0_3_0_1_3_5_1_1_0': 14,  # SS=0.3210, N=148, old=14
    '7_0_3_0_1_3_5_1_2_0': 15,  # SS=0.3243, N=170, old=15
    '7_0_3_0_1_3_5_1_3_0': 10,  # SS=0.4163, N=74, old=10
    '7_0_3_0_1_3_6_0_1_0': 20,  # SS=0.3305, N=335, old=20
    '7_0_3_0_1_3_6_0_2_0': 21,  # SS=0.3270, N=330, old=21
    '7_0_3_0_1_3_6_0_3_0': 16,  # SS=0.2631, N=176, old=16
    '7_0_3_0_1_3_6_0_4_0': 11,  # SS=0.3229, N=81, old=11
    '7_0_3_0_1_3_6_1_1_0': 20,  # SS=0.2916, N=275, old=20
    '7_0_3_0_1_3_6_1_2_0': 25,  # SS=0.2764, N=473, old=25
    '7_0_3_0_1_3_6_1_3_0': 17,  # SS=0.3418, N=265, old=17
    '7_0_3_0_1_3_6_1_4_0': 12,  # SS=0.3094, N=140, old=12
    '7_0_3_1_1_0_0_0_0_0': 17,  # SS=0.5942, N=556, old=17
    '7_0_3_1_1_0_6_1_0_0': 11,  # SS=0.4921, N=195, old=11
    '7_0_3_1_1_0_6_2_0_0': 38,  # SS=0.4007, N=2594, old=38
    '7_0_3_1_1_0_6_3_0_0': 44,  # SS=0.3784, N=3516, old=44
    '7_0_3_1_1_0_7_2_0_0': 20,  # SS=0.4353, N=762, old=20
    '7_0_3_1_1_0_7_3_0_0': 15,  # SS=0.4028, N=342, old=15
    '7_0_3_1_1_1_6_3_0_0': 11,  # SS=0.2693, N=191, old=11
    '7_0_3_1_1_2_5_2_1_0': 54,  # SS=0.2078, N=2267, old=54
    '7_0_3_1_1_2_5_2_2_0': 53,  # SS=0.2522, N=2092, old=53
    '7_0_3_1_1_2_5_2_3_0': 40,  # SS=0.2811, N=1231, old=40
    '7_0_3_1_1_2_5_2_4_0': 29,  # SS=0.3485, N=658, old=29
    '7_0_3_1_1_2_5_2_5_0': 13,  # SS=0.4640, N=145, old=13
    '7_0_3_1_1_2_6_2_0_0': 17,  # SS=0.2148, N=518, old=17
    '7_0_3_1_1_2_6_2_1_0': 60,  # SS=0.2174, N=2799, old=60
    '7_0_3_1_1_2_6_2_2_0': 61,  # SS=0.2210, N=2729, old=61
    '7_0_3_1_1_2_6_2_3_0': 41,  # SS=0.2355, N=1425, old=41
    '7_0_3_1_1_2_6_2_4_0': 28,  # SS=0.3093, N=569, old=28
    '7_0_3_1_1_2_6_2_5_0': 13,  # SS=0.3917, N=107, old=13
    '7_0_3_1_1_2_6_3_0_0': 21,  # SS=0.2069, N=737, old=21
    '7_0_3_1_1_3_5_2_1_0': 34,  # SS=0.2446, N=875, old=34
    '7_0_3_1_1_3_5_2_2_0': 27,  # SS=0.2627, N=556, old=27
    '7_0_3_1_1_3_5_2_3_0': 18,  # SS=0.3058, N=251, old=18
    '7_0_3_1_1_3_5_2_4_0': 11,  # SS=0.3848, N=89, old=11
    '7_0_3_1_1_3_5_3_1_0': 23,  # SS=0.2121, N=430, old=23
    '7_0_3_1_1_3_5_3_2_0': 24,  # SS=0.2371, N=407, old=24
    '7_0_3_1_1_3_5_3_3_0': 14,  # SS=0.2744, N=172, old=14
    '7_0_3_1_1_3_5_3_4_0': 11,  # SS=0.4461, N=94, old=11
    '7_0_3_1_1_3_6_2_0_0': 13,  # SS=0.2383, N=308, old=13
    '7_0_3_1_1_3_6_2_1_0': 19,  # SS=0.3750, N=303, old=19
    '7_0_3_1_1_3_6_2_2_0': 13,  # SS=0.3000, N=125, old=13
    '7_0_3_1_1_3_6_3_0_0': 15,  # SS=0.2420, N=382, old=15
    '7_0_4_0_0_0_6_2_0_0': 15,  # SS=0.4071, N=460, old=15
    '7_0_4_0_0_0_6_3_0_0': 14,  # SS=0.3563, N=370, old=14
    '7_0_4_0_0_0_8_4_0_0': 14,  # SS=0.4503, N=403, old=14
    '7_0_4_0_0_1_0_0_0_0': 30,  # SS=0.4287, N=695, old=30
    '7_0_4_0_0_2_0_0_0_0': 28,  # SS=0.2538, N=613, old=28
    '7_0_4_0_0_3_0_0_0_0': 24,  # SS=0.3378, N=520, old=24
    '7_0_4_0_1_0_6_2_0_0': 23,  # SS=0.4327, N=960, old=23
    '7_0_4_0_1_0_6_3_0_0': 10,  # SS=0.5316, N=116, old=10
    '7_0_4_0_1_2_6_0_2_0': 10,  # SS=0.4366, N=72, old=10
    '7_0_4_0_1_2_6_0_3_0': 12,  # SS=0.4220, N=96, old=12
    '7_0_4_0_1_2_6_2_0_0': 11,  # SS=0.3136, N=202, old=11
    '7_0_4_0_1_3_5_0_1_0': 13,  # SS=0.4233, N=136, old=13
    '7_0_4_0_1_3_5_0_2_0': 13,  # SS=0.4709, N=95, old=13
    '7_0_4_0_1_3_5_0_3_0': 11,  # SS=0.4462, N=76, old=11
    '7_0_4_0_1_3_6_0_1_0': 28,  # SS=0.3613, N=615, old=28
    '7_0_4_0_1_3_6_0_2_0': 28,  # SS=0.3142, N=680, old=28
    '7_0_4_0_1_3_6_0_3_0': 24,  # SS=0.3648, N=409, old=24
    '7_0_4_0_1_3_6_0_4_0': 15,  # SS=0.4351, N=196, old=15
    '7_1_3_0_0_3_0_1_0_0': 16,  # SS=0.4495, N=176, old=16
    '8_-1_3_0_0_1_0_0_0_0': 17,  # SS=0.4665, N=207, old=17
    '8_0_3_0_0_0_0_0_0_0': 14,  # SS=0.6299, N=370, old=14
    '8_0_3_0_0_0_6_1_0_0': 14,  # SS=0.5374, N=369, old=14
    '8_0_3_0_0_0_6_2_0_0': 30,  # SS=0.4130, N=1745, old=30
    '8_0_3_0_0_0_6_3_0_0': 68,  # SS=0.2988, N=8636, old=68
    '8_0_3_0_0_0_7_2_0_0': 28,  # SS=0.4224, N=1417, old=28
    '8_0_3_0_0_0_7_3_0_0': 19,  # SS=0.3999, N=681, old=19
    '8_0_3_0_0_0_8_1_0_0': 24,  # SS=0.4495, N=1165, old=24
    '8_0_3_0_0_0_8_2_0_0': 11,  # SS=0.4877, N=239, old=11
    '8_0_3_0_0_0_8_4_0_0': 16,  # SS=0.3535, N=427, old=16
    '8_0_3_0_0_1_0_0_0_0': 141,  # SS=0.2749, N=15528, old=141
    '8_0_3_0_0_1_0_1_0_0': 54,  # SS=0.2593, N=2274, old=54
    '8_0_3_0_0_1_6_3_0_0': 28,  # SS=0.2429, N=1318, old=28
    '8_0_3_0_0_2_0_0_0_0': 35,  # SS=0.3200, N=948, old=35
    '8_0_3_0_0_2_0_1_0_0': 72,  # SS=0.2408, N=3818, old=72
    '8_0_3_0_0_2_0_2_0_0': 24,  # SS=0.2459, N=450, old=24
    '8_0_3_0_0_2_6_2_0_0': 14,  # SS=0.2902, N=344, old=14
    '8_0_3_0_0_2_6_3_0_0': 32,  # SS=0.2352, N=1836, old=32
    '8_0_3_0_0_2_7_2_0_0': 13,  # SS=0.2136, N=275, old=13
    '8_0_3_0_0_2_8_1_0_0': 12,  # SS=0.2470, N=233, old=12
    '8_0_3_0_0_3_6_2_0_0': 12,  # SS=0.3177, N=212, old=12
    '8_0_3_0_0_3_6_3_0_0': 25,  # SS=0.2353, N=1074, old=25
    '8_0_3_0_0_3_8_1_0_0': 11,  # SS=0.2631, N=167, old=11
    '8_0_3_0_1_2_5_0_1_0': 10,  # SS=0.5536, N=79, old=10
    '8_0_3_0_1_2_5_1_1_0': 11,  # SS=0.5094, N=88, old=11
    '8_0_3_0_1_2_6_1_1_0': 12,  # SS=0.3809, N=119, old=12
    '8_0_3_1_1_0_6_3_0_0': 12,  # SS=0.4101, N=206, old=12
    '8_0_3_1_1_2_5_2_1_0': 20,  # SS=0.3285, N=293, old=20
    '8_0_3_1_1_2_5_2_2_0': 19,  # SS=0.2912, N=231, old=19
    '8_0_3_1_1_2_5_2_3_0': 16,  # SS=0.4007, N=176, old=16
    '8_0_3_1_1_2_5_2_4_0': 11,  # SS=0.5454, N=95, old=11
    '8_0_4_0_0_0_6_2_0_0': 21,  # SS=0.4382, N=648, old=21
    '8_0_4_0_0_0_6_3_0_0': 17,  # SS=0.4618, N=558, old=17
    '8_0_4_0_0_1_0_0_0_0': 55,  # SS=0.3269, N=2338, old=55
    '8_0_4_0_0_2_0_0_0_0': 31,  # SS=0.3099, N=726, old=31
    '8_0_4_0_1_0_6_2_0_0': 12,  # SS=0.4700, N=300, old=12
    '8_0_4_0_1_2_5_0_1_0': 11,  # SS=0.4544, N=91, old=11
    '8_0_4_0_1_2_6_0_1_0': 13,  # SS=0.4072, N=124, old=13
    '8_0_4_0_1_2_6_0_2_0': 13,  # SS=0.5428, N=145, old=13
    '8_0_4_0_1_2_6_0_3_0': 18,  # SS=0.5324, N=250, old=18
    '8_0_4_0_1_2_6_0_4_0': 12,  # SS=0.5116, N=152, old=12
}



def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def get_training_config(config_path, model_name, dataset):
    with open(config_path, "r") as conf:
        full_config = yaml.load(conf, Loader=yaml.FullLoader)
    dataset_specific_config = full_config["global"]
    model_specific_config = full_config[dataset][model_name]

    if model_specific_config is not None:
        specific_config = dict(dataset_specific_config, **model_specific_config)
    else:
        specific_config = dataset_specific_config

    specific_config["model_name"] = model_name
    return specific_config


def check_writable(path, overwrite=True):
    if not os.path.exists(path):
        os.makedirs(path)
    elif overwrite:
        shutil.rmtree(path)
        os.makedirs(path)
    else:
        pass


def check_readable(path):
    if not os.path.exists(path):
        raise ValueError(f"No such file or directory! {path}")


def timetz(*args):
    tz = pytz.timezone("US/Pacific")
    return datetime.now(tz).timetuple()


def get_logger(filename, console_log=False, log_level=logging.INFO):
    tz = pytz.timezone("US/Pacific")
    log_time = datetime.now(tz).strftime("%b%d_%H_%M_%S")
    logger = logging.getLogger(__name__)
    logger.propagate = False  # avoid duplicate logging
    logger.setLevel(log_level)

    # Clean logger first to avoid duplicated handlers
    for hdlr in logger.handlers[:]:
        logger.removeHandler(hdlr)

    file_handler = logging.FileHandler(filename)
    formatter = logging.Formatter("%(asctime)s: %(message)s", datefmt="%b%d %H-%M-%S")
    formatter.converter = timetz
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)

    if console_log:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
    return logger


def idx_split(idx, ratio, seed=0, train_or_infer=None):
    """
    randomly split idx into two portions with ratio% elements and (1 - ratio)% elements
    """
    set_seed(seed)
    n = len(idx)   # idx starts from 40
    cut = int(n * ratio)  # n 8000, cut 1600, ratio 0.2
    # print(f"n {n}, cut {cut}, ratio {ratio}") # n 8000, cut 1600, ratio 0.2
    if train_or_infer == "train":
        idx_idx_shuffle = torch.randperm(n)
        idx1_idx, idx2_idx = idx_idx_shuffle[:cut], idx_idx_shuffle[cut:]
    elif train_or_infer == "infer":
        idx_idx_list = list(range(n))
        idx1_idx, idx2_idx = idx_idx_list[:cut], idx_idx_list[cut:]
    idx1, idx2 = idx[idx1_idx], idx[idx2_idx]
    # assert((torch.cat([idx1, idx2]).sort()[0] == idx.sort()[0]).all())
    return idx1, idx2  # idx1 is test_ind


def graph_split(idx_train, idx_val, idx_test, rate, seed, train_or_infer):
    """
    Args:
        The original setting was transductive. Full graph is observed, and idx_train takes up a small portion.
        Split the graph by further divide idx_test into [idx_test_tran, idx_test_ind].
        rate = idx_test_ind : idx_test (how much test to hide for the inductive evaluation)

        Ex. Ogbn-products
        loaded     : train : val : test = 8 : 2 : 90, rate = 0.2
        after split: train : val : test_tran : test_ind = 8 : 2 : 72 : 18

    Return:
        Indices start with 'obs_' correspond to the node indices within the observed subgraph,
        where as indices start directly with 'idx_' correspond to the node indices in the original graph
    """
    idx_test_ind, idx_test_tran = idx_split(idx_test, rate, seed, train_or_infer)

    idx_obs = torch.cat([idx_train, idx_val])
    N1, N2 = idx_train.shape[0], idx_val.shape[0]
    obs_idx_all = torch.arange(idx_obs.shape[0])
    obs_idx_train = obs_idx_all[:N1]
    obs_idx_val = obs_idx_all[N1 : N1 + N2]
    obs_idx_test = idx_test

    # print(f"obs_idx_train {obs_idx_train}")
    # print(f"obs_idx_train {obs_idx_train.shape}")
    # print(f"obs_idx_val {obs_idx_val}")
    # print(f"obs_idx_val {obs_idx_val.shape}")
    # print(f"obs_idx_test {obs_idx_test}")
    # print(f"obs_idx_test {obs_idx_test.shape}")
    # print(f"idx_test_ind {idx_test_ind}")
    # print(f"idx_test_ind {idx_test_ind.shape}")
    idx_test_ind = torch.tensor(list(range(N1 + N2 + N2, N1 + N2 + N2 + N2 + 1)))
    return obs_idx_train, obs_idx_val, obs_idx_test, obs_idx_all, idx_test_ind


def get_evaluator(dataset):
    if dataset in CPF_data + NonHom_data + BGNN_data:

        def evaluator(out, labels):
            pred = out.argmax(1)
            return pred.eq(labels).float().mean().item()

    elif dataset in OGB_data:
        ogb_evaluator = Evaluator(dataset)

        def evaluator(out, labels):
            pred = out.argmax(1, keepdim=True)
            input_dict = {"y_true": labels.unsqueeze(1), "y_pred": pred}
            return ogb_evaluator.eval(input_dict)["acc"]

    else:
        raise ValueError("Unknown dataset")

    return evaluator


def get_evaluator(dataset):
    def evaluator(out, labels):
        pred = out.argmax(1)
        return pred.eq(labels).float().mean().item()

    return evaluator


def compute_min_cut_loss(g, out):
    out = out.to("cpu")
    g = g.to("cpu")
    S = out.exp()
    A = g.adj().to_dense()
    D = g.in_degrees().float().diag()
    print(S.device, A.device, D.device)
    min_cut = (
        torch.matmul(torch.matmul(S.transpose(1, 0), A), S).trace()
        / torch.matmul(torch.matmul(S.transpose(1, 0), D), S).trace()
    )
    return min_cut.item()


def feature_prop(feats, g, k):
    """
    Augment node feature by propagating the node features within k-hop neighborhood.
    The propagation is done in the SGC fashion, i.e. hop by hop and symmetrically normalized by node degrees.
    """
    assert feats.shape[0] == g.num_nodes()

    degs = g.in_degrees().float().clamp(min=1)
    norm = torch.pow(degs, -0.5).unsqueeze(1)

    # compute (D^-1/2 A D^-1/2)^k X
    for _ in range(k):
        feats = feats * norm
        g.ndata["h"] = feats
        g.update_all(fn.copy_u("h", "m"), fn.sum("m", "h"))
        feats = g.ndata.pop("h")
        feats = feats * norm

    return feats
